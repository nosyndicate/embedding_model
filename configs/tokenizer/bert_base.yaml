# BERT base tokenizer configuration
# Used by tokenize_fineweb.py script and stored in shard headers
tokenizer_name_or_path: "bert-base-uncased"
max_length: 512

# Special token IDs (for reference)
vocab_size: 30522
pad_token_id: 0
cls_token_id: 101
sep_token_id: 102
mask_token_id: 103
unk_token_id: 100
