# Pre-tokenized dataset configuration
_target_: embedding_trainer.data.datasets.pretokenized.PreTokenizedDataset
_recursive_: false

config:
  _target_: embedding_trainer.data.datasets.pretokenized.PreTokenizedConfig
  data_dir: "${hydra:runtime.cwd}/data/fineweb_edu_100bt"
  max_seq_length: 512
  shuffle: true
  stride: null  # Defaults to max_seq_length (no overlap)
  split: train
  seed: 42

  # Document boundary handling
  packing_mode: flat  # flat | doc_aware

  # Sequence sampling strategy (for flat mode)
  # - fixed: Always start at offset 0
  # - epoch_offset: Rotate starting offset each epoch (4x data variety)
  # - random: Sample random offsets (same count, random positions)
  # - shuffle_indices: Shuffle non-overlapping sequence indices
  sampling_strategy: epoch_offset
  num_offset_phases: 4
